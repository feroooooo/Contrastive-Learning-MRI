{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_stl10 = r'C:\\Custom\\DataSet\\STL10'\n",
    "# data_dir_cifar10 = r'C:\\Custom\\DataSet\\CIFAR10'\n",
    "\n",
    "# def get_stl10_data_loaders(download, shuffle=False, batch_size=256):\n",
    "#   train_dataset = datasets.STL10(data_dir_stl10, split='train', download=download,\n",
    "#                                   transform=transforms.ToTensor())\n",
    "\n",
    "#   train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "#                             num_workers=0, drop_last=False, shuffle=shuffle)\n",
    "  \n",
    "#   test_dataset = datasets.STL10(data_dir_stl10, split='test', download=download,\n",
    "#                                   transform=transforms.ToTensor())\n",
    "\n",
    "#   test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
    "#                             num_workers=10, drop_last=False, shuffle=shuffle)\n",
    "#   return train_loader, test_loader\n",
    "\n",
    "# def get_cifar10_data_loaders(download, shuffle=False, batch_size=256):\n",
    "#   train_dataset = datasets.CIFAR10(data_dir_cifar10, train=True, download=download,\n",
    "#                                   transform=transforms.ToTensor())\n",
    "\n",
    "#   train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "#                             num_workers=0, drop_last=False, shuffle=shuffle)\n",
    "  \n",
    "#   test_dataset = datasets.CIFAR10(data_dir_cifar10, train=False, download=download,\n",
    "#                                   transform=transforms.ToTensor())\n",
    "\n",
    "#   test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
    "#                             num_workers=10, drop_last=False, shuffle=shuffle)\n",
    "#   return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mri_dataset import ADNIDataset\n",
    "from monai.transforms import *\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size=256):\n",
    "    dataset_dir = r\"E:\\Data\\ADNI\\adni-fnirt-corrected\"\n",
    "    csv_path = r\"E:\\Data\\ADNI\\pheno_ADNI_longitudinal_new.csv\"\n",
    "    size = 100\n",
    "    data_transforms = Compose([\n",
    "        RandRotate90(prob=0.5, spatial_axes=[1, 2]),\n",
    "        RandFlip(prob=0.5, spatial_axis=0),\n",
    "        \n",
    "        RandAdjustContrast(prob=0.5),\n",
    "        RandGaussianNoise(prob=0.3),\n",
    "        RandAffine(prob=0.5, translate_range=10, scale_range=(0.9, 1.1), rotate_range=45),\n",
    "        \n",
    "        Resize(spatial_size=[size, size, size]),\n",
    "        NormalizeIntensity(nonzero=True, channel_wise=True),\n",
    "    ])\n",
    "    dataset = ADNIDataset(data_dir=dataset_dir, csv_path=csv_path, transform=data_transforms)\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(dataset_size * 0.7)\n",
    "    test_size = dataset_size - train_size\n",
    "    print('dataset_size:', dataset_size)\n",
    "    print('train_size:', train_size)\n",
    "    print('test_size:', test_size)\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './runs/Mar20_18-41-01_DESKTOP-ZERO'\n",
    "\n",
    "with open(os.path.join(log_dir, 'config.yml')) as file:\n",
    "  config = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config['arch'] == 'resnet18':\n",
    "#   model = torchvision.models.resnet18(num_classes=10).to(device)\n",
    "# elif config['arch'] == 'resnet50':\n",
    "#   model = torchvision.models.resnet50(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Simple3DCNN, VoxVGG, VoxResNet\n",
    "\n",
    "if config['arch'] == 'simple':\n",
    "    model = Simple3DCNN(class_nums=3)\n",
    "elif config['arch'] == 'vgg':\n",
    "    model = VoxVGG(class_nums=3)\n",
    "elif config['arch'] == 'resnet':\n",
    "    model = VoxResNet(class_nums=3)\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filename = 'checkpoint_{:04}.pth.tar'.format(config['epochs'])\n",
    "checkpoint_path = os.path.join(log_dir, checkpoint_filename)\n",
    "print(checkpoint_path)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "state_dict = checkpoint['state_dict']\n",
    "print('keys:', list(state_dict.keys()))\n",
    "\n",
    "for k in list(state_dict.keys()):\n",
    "  if k.startswith('backbone.'):\n",
    "    if k.startswith('backbone') and not k.startswith('backbone.last_fc'):\n",
    "      # remove prefix\n",
    "      state_dict[k[len(\"backbone.\"):]] = state_dict[k]\n",
    "    else:\n",
    "      print(k)\n",
    "  del state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.load_state_dict(state_dict, strict=False)\n",
    "print(log.missing_keys)\n",
    "assert log.missing_keys == ['last_fc.weight', 'last_fc.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config['dataset_name'] == 'cifar10':\n",
    "#   train_loader, test_loader = get_cifar10_data_loaders(download=True)\n",
    "# elif config['dataset_name'] == 'stl10':\n",
    "#   train_loader, test_loader = get_stl10_data_loaders(download=True)\n",
    "# print(\"Dataset:\", config['dataset_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset_name'] == 'mri':\n",
    "    train_loader, test_loader = get_data_loaders(batch_size=config['batch_size'] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers but the last fc\n",
    "for name, param in model.named_parameters():\n",
    "    if name not in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)\n",
    "\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "assert len(parameters) == 2  # fc.weight, fc.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0008)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    top1_train_accuracy = 0\n",
    "    print(f'epoch:{epoch + 1}, train')\n",
    "    for counter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        top1 = accuracy(logits, y_batch, topk=(1,))\n",
    "        top1_train_accuracy += top1[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    top1_train_accuracy /= counter + 1\n",
    "    top1_accuracy = 0\n",
    "    top3_accuracy = 0\n",
    "    print(f'epoch:{epoch + 1}, test')\n",
    "    for counter, (x_batch, y_batch) in enumerate(test_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = model(x_batch)\n",
    "\n",
    "        top1, top3 = accuracy(logits, y_batch, topk=(1, 3))\n",
    "        top1_accuracy += top1[0]\n",
    "        top3_accuracy += top3[0]\n",
    "\n",
    "    top1_accuracy /= counter + 1\n",
    "    top3_accuracy /= counter + 1\n",
    "    print(\n",
    "        f\"Epoch {epoch}\\tTop1 Train accuracy {top1_train_accuracy.item()}\\tTop1 Test accuracy: {top1_accuracy.item()}\\tTop3 test acc: {top3_accuracy.item()}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simclr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
