{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from medcam import medcam\n",
    "from monai import transforms\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from model import VoxResNet, VGG3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.device' object has no attribute '_apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVGG3D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model = VoxResNet().to(device)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# checkpoint_path = r\"C:\\Users\\17993\\Downloads\\best_epoch17.pth\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# epoch = checkpoint['epoch']\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(f\"Model loaded successfully with accuracy: {100.*accuracy:.2f}%, loss: {validation_loss:.6f}, at epoch: {epoch}\")\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m medcam\u001b[38;5;241m.\u001b[39minject(model, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_maps\u001b[39m\u001b[38;5;124m'\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgcam\u001b[39m\u001b[38;5;124m'\u001b[39m, save_maps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Custom\\Software\\Environment\\anaconda3\\envs\\mri\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m(convert)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'torch.device' object has no attribute '_apply'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VGG3D().to(device)\n",
    "# model = VoxResNet().to(device)\n",
    "\n",
    "# checkpoint_path = r\"C:\\Users\\17993\\Downloads\\best_epoch17.pth\"\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "\n",
    "# # 加载模型权重\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# # 获取保存的accuracy, loss和epoch\n",
    "# accuracy = checkpoint['accuracy']\n",
    "# validation_loss = checkpoint['loss']\n",
    "# epoch = checkpoint['epoch']\n",
    "# print(f\"Model loaded successfully with accuracy: {100.*accuracy:.2f}%, loss: {validation_loss:.6f}, at epoch: {epoch}\")\n",
    "\n",
    "model = medcam.inject(model, output_dir='attention_maps', backend='gcam', save_maps=False, return_attention=True, layer='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(spatial_size=[110, 110, 110]),\n",
    "    transforms.NormalizeIntensity(nonzero=True, channel_wise=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = r\"C:\\Custom\\DataSet\\ADNI_预处理后\\Image\\brain_adni_0021_I196077_fsld.nii.gz\"\n",
    "nii_img = nib.load(img_path).get_fdata()\n",
    "nii_img = nii_img.astype(np.float32)\n",
    "original_img = nii_img\n",
    "nii_img = torch.from_numpy(nii_img)\n",
    "nii_img = nii_img.unsqueeze(0)\n",
    "nii_img = transform(nii_img)\n",
    "nii_img = nii_img.unsqueeze(0)\n",
    "nii_img = nii_img.as_tensor()\n",
    "print(nii_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 进行预测\n",
    "    outputs, attention_map = model(nii_img)\n",
    "    # 输出处理，获取预测结果\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    attention_map = attention_map.detach().cpu().numpy()\n",
    "attention_map = np.squeeze(attention_map)\n",
    "print('predicted:', predicted)\n",
    "print('attention_map shape:', attention_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imsave('./attention_maps/test.jpg', attention_map[40], cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_idx = 50\n",
    "print(original_img[:,:,slice_idx].shape)\n",
    "plt.imshow(np.flipud(original_img[:,:,slice_idx].T), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(110):\n",
    "    for j in range(110):\n",
    "        for k in range(110):\n",
    "            if attention_map[i][j][k] == 0:\n",
    "                cnt += 1\n",
    "            else:\n",
    "                print(\"test\")\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.flipud(attention_map[:,:,slice_idx].T), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
